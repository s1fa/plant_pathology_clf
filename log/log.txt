> step 20: batch_loss 0.4092 | norm 0.7565 | lr 0.000995
> step 40: batch_loss 0.4492 | norm 0.6340 | lr 0.001990
> step 60: batch_loss 0.4886 | norm 0.8004 | lr 0.002985
> step 80: batch_loss 0.4130 | norm 0.7987 | lr 0.003980
> step 100: batch_loss 0.4733 | norm 1.1713 | lr 0.004975
>> step 100: val_loss 0.4505
> step 120: batch_loss 0.4727 | norm 0.7507 | lr 0.005970
> step 140: batch_loss 0.4973 | norm 1.0205 | lr 0.006965
> step 160: batch_loss 0.4561 | norm 0.9175 | lr 0.007960
> step 180: batch_loss 0.4270 | norm 0.5156 | lr 0.008955
> step 200: batch_loss 0.4333 | norm 0.3337 | lr 0.009950
>> step 200: val_loss 0.4360
> step 220: batch_loss 0.4723 | norm 0.4975 | lr 0.010000
> step 240: batch_loss 0.4450 | norm 0.3416 | lr 0.009999
> step 260: batch_loss 0.4919 | norm 0.3122 | lr 0.009998
> step 280: batch_loss 0.4357 | norm 0.2634 | lr 0.009996
> step 300: batch_loss 0.4288 | norm 0.3109 | lr 0.009994
>> step 300: val_loss 0.4344
> step 320: batch_loss 0.4160 | norm 0.2083 | lr 0.009991
> step 340: batch_loss 0.4205 | norm 0.2834 | lr 0.009987
> step 360: batch_loss 0.4516 | norm 0.2046 | lr 0.009983
> step 380: batch_loss 0.4149 | norm 0.1889 | lr 0.009979
> step 400: batch_loss 0.4323 | norm 0.1889 | lr 0.009974
>> step 400: val_loss 0.4316
> step 420: batch_loss 0.4365 | norm 0.1221 | lr 0.009968
> step 440: batch_loss 0.4199 | norm 0.1258 | lr 0.009962
> step 460: batch_loss 0.4581 | norm 0.1957 | lr 0.009956
> step 480: batch_loss 0.4088 | norm 0.1242 | lr 0.009949
> step 500: batch_loss 0.4301 | norm 0.2269 | lr 0.009941
>> step 500: val_loss 0.4297
> step 520: batch_loss 0.4249 | norm 0.1039 | lr 0.009933
> step 540: batch_loss 0.4265 | norm 0.0725 | lr 0.009924
> step 560: batch_loss 0.4189 | norm 0.1177 | lr 0.009915
> step 580: batch_loss 0.4051 | norm 0.0402 | lr 0.009906
> step 600: batch_loss 0.4371 | norm 0.0440 | lr 0.009895
>> step 600: val_loss 0.4328
> step 620: batch_loss 0.4394 | norm 0.1020 | lr 0.009885
> step 640: batch_loss 0.4336 | norm 0.0905 | lr 0.009873
> step 660: batch_loss 0.4591 | norm 0.0736 | lr 0.009862
> step 680: batch_loss 0.4274 | norm 0.0963 | lr 0.009849
> step 700: batch_loss 0.4576 | norm 0.0826 | lr 0.009837
>> step 700: val_loss 0.4275
> step 720: batch_loss 0.4196 | norm 0.0931 | lr 0.009823
> step 740: batch_loss 0.4267 | norm 0.0737 | lr 0.009810
> step 760: batch_loss 0.4421 | norm 0.0443 | lr 0.009795
> step 780: batch_loss 0.3960 | norm 0.0578 | lr 0.009781
> step 800: batch_loss 0.4227 | norm 0.0850 | lr 0.009765
>> step 800: val_loss 0.4355
> step 820: batch_loss 0.4574 | norm 0.0654 | lr 0.009749
> step 840: batch_loss 0.4045 | norm 0.0916 | lr 0.009733
> step 860: batch_loss 0.4166 | norm 0.0672 | lr 0.009716
> step 880: batch_loss 0.4292 | norm 1.1218 | lr 0.009699
> step 900: batch_loss 0.4299 | norm 0.1180 | lr 0.009681
>> step 900: val_loss 0.4385
> step 920: batch_loss 0.4599 | norm 0.1188 | lr 0.009663
> step 940: batch_loss 0.4483 | norm 0.1556 | lr 0.009644
> step 960: batch_loss 0.4490 | norm 0.0841 | lr 0.009625
> step 980: batch_loss 0.4248 | norm 0.1765 | lr 0.009605
> step 1000: batch_loss 0.4491 | norm 0.0875 | lr 0.009585
>> step 1000: val_loss 0.4403
> step 1020: batch_loss 0.4613 | norm 0.0706 | lr 0.009564
> step 1040: batch_loss 0.4428 | norm 0.5279 | lr 0.009543
> step 1060: batch_loss 0.4501 | norm 0.0546 | lr 0.009522
> step 1080: batch_loss 0.4127 | norm 4.1362 | lr 0.009500
> step 1100: batch_loss 0.4594 | norm 0.0773 | lr 0.009477
>> step 1100: val_loss 0.4408
> step 1120: batch_loss 0.4337 | norm 0.5579 | lr 0.009454
> step 1140: batch_loss 0.4232 | norm 0.0810 | lr 0.009430
> step 1160: batch_loss 0.4436 | norm 0.1340 | lr 0.009406
> step 1180: batch_loss 0.4227 | norm 0.0749 | lr 0.009382
> step 1200: batch_loss 0.4214 | norm 0.1615 | lr 0.009357
>> step 1200: val_loss 0.4341
> step 1220: batch_loss 0.4293 | norm 0.1305 | lr 0.009332
> step 1240: batch_loss 0.4413 | norm 0.0997 | lr 0.009306
> step 1260: batch_loss 0.4296 | norm 0.0680 | lr 0.009280
> step 1280: batch_loss 0.4417 | norm 0.0703 | lr 0.009253
> step 1300: batch_loss 0.4356 | norm 0.1047 | lr 0.009226
>> step 1300: val_loss 0.4332
> step 1320: batch_loss 0.3953 | norm 0.0602 | lr 0.009198
> step 1340: batch_loss 0.4280 | norm 0.0554 | lr 0.009170
> step 1360: batch_loss 0.4455 | norm 0.0564 | lr 0.009142
> step 1380: batch_loss 0.4293 | norm 0.0782 | lr 0.009113
> step 1400: batch_loss 0.4311 | norm 0.0650 | lr 0.009084
>> step 1400: val_loss 0.4302
> step 1420: batch_loss 0.4184 | norm 0.0922 | lr 0.009054
> step 1440: batch_loss 0.4240 | norm 0.0881 | lr 0.009024
> step 1460: batch_loss 0.4314 | norm 0.1007 | lr 0.008994
> step 1480: batch_loss 0.4395 | norm 0.0966 | lr 0.008963
> step 1500: batch_loss 0.4221 | norm 0.0726 | lr 0.008931
>> step 1500: val_loss 0.4317
> step 1520: batch_loss 0.4352 | norm 0.0757 | lr 0.008900
> step 1540: batch_loss 0.4220 | norm 0.1101 | lr 0.008867
> step 1560: batch_loss 0.4209 | norm 0.0504 | lr 0.008835
> step 1580: batch_loss 0.4364 | norm 0.0579 | lr 0.008802
> step 1600: batch_loss 0.4329 | norm 0.0725 | lr 0.008769
>> step 1600: val_loss 0.4266
> step 1620: batch_loss 0.4017 | norm 0.0671 | lr 0.008735
> step 1640: batch_loss 0.4252 | norm 0.0676 | lr 0.008701
> step 1660: batch_loss 0.4260 | norm 0.1009 | lr 0.008666
> step 1680: batch_loss 0.4050 | norm 0.0419 | lr 0.008632
> step 1700: batch_loss 0.4537 | norm 0.0427 | lr 0.008596
>> step 1700: val_loss 0.4274
> step 1720: batch_loss 0.4467 | norm 0.0948 | lr 0.008561
> step 1740: batch_loss 0.4380 | norm 0.1633 | lr 0.008525
> step 1760: batch_loss 0.4259 | norm 0.2487 | lr 0.008489
> step 1780: batch_loss 0.4021 | norm 0.0665 | lr 0.008452
> step 1800: batch_loss 0.4255 | norm 0.0516 | lr 0.008415
>> step 1800: val_loss 0.4300
> step 1820: batch_loss 0.4165 | norm 0.1019 | lr 0.008378
> step 1840: batch_loss 0.4013 | norm 0.0879 | lr 0.008340
> step 1860: batch_loss 0.3893 | norm 0.1115 | lr 0.008302
> step 1880: batch_loss 0.3968 | norm 0.1199 | lr 0.008264
> step 1900: batch_loss 0.4392 | norm 0.1446 | lr 0.008225
>> step 1900: val_loss 0.4268
> step 1920: batch_loss 0.4174 | norm 0.0630 | lr 0.008186
> step 1940: batch_loss 0.4454 | norm 0.1201 | lr 0.008147
> step 1960: batch_loss 0.4176 | norm 0.0849 | lr 0.008107
> step 1980: batch_loss 0.4428 | norm 0.1153 | lr 0.008068
> step 2000: batch_loss 0.4304 | norm 0.0639 | lr 0.008027
>> step 2000: val_loss 0.4270
> step 2020: batch_loss 0.4551 | norm 0.0789 | lr 0.007987
> step 2040: batch_loss 0.4319 | norm 0.0830 | lr 0.007946
> step 2060: batch_loss 0.4321 | norm 0.0639 | lr 0.007905
> step 2080: batch_loss 0.4217 | norm 0.1001 | lr 0.007864
> step 2100: batch_loss 0.4018 | norm 0.0899 | lr 0.007822
>> step 2100: val_loss 0.4257
> step 2120: batch_loss 0.4169 | norm 0.0814 | lr 0.007780
> step 2140: batch_loss 0.4648 | norm 0.0675 | lr 0.007738
> step 2160: batch_loss 0.4056 | norm 0.1033 | lr 0.007696
> step 2180: batch_loss 0.4435 | norm 0.1379 | lr 0.007653
> step 2200: batch_loss 0.4012 | norm 0.0831 | lr 0.007610
>> step 2200: val_loss 0.4260
> step 2220: batch_loss 0.4399 | norm 0.0815 | lr 0.007567
> step 2240: batch_loss 0.4280 | norm 0.0750 | lr 0.007523
> step 2260: batch_loss 0.4422 | norm 0.0692 | lr 0.007480
> step 2280: batch_loss 0.4403 | norm 0.0887 | lr 0.007436
> step 2300: batch_loss 0.3989 | norm 0.0759 | lr 0.007392
>> step 2300: val_loss 0.4252
> step 2320: batch_loss 0.3809 | norm 0.1012 | lr 0.007347
> step 2340: batch_loss 0.3869 | norm 0.0575 | lr 0.007303
> step 2360: batch_loss 0.4367 | norm 0.0617 | lr 0.007258
> step 2380: batch_loss 0.3980 | norm 0.0631 | lr 0.007213
> step 2400: batch_loss 0.4384 | norm 0.0518 | lr 0.007168
>> step 2400: val_loss 0.4234
> step 2420: batch_loss 0.3993 | norm 0.1084 | lr 0.007123
> step 2440: batch_loss 0.4159 | norm 0.1077 | lr 0.007077
> step 2460: batch_loss 0.4388 | norm 0.1099 | lr 0.007031
> step 2480: batch_loss 0.4028 | norm 0.1141 | lr 0.006985
> step 2500: batch_loss 0.4294 | norm 0.1167 | lr 0.006939
>> step 2500: val_loss 0.4192
> step 2520: batch_loss 0.4049 | norm 0.0953 | lr 0.006893
> step 2540: batch_loss 0.4085 | norm 0.0641 | lr 0.006846
> step 2560: batch_loss 0.4171 | norm 0.1107 | lr 0.006800
> step 2580: batch_loss 0.4596 | norm 0.0690 | lr 0.006753
> step 2600: batch_loss 0.4052 | norm 0.1022 | lr 0.006706
>> step 2600: val_loss 0.4194
> step 2620: batch_loss 0.4127 | norm 0.0700 | lr 0.006659
> step 2640: batch_loss 0.4016 | norm 0.0732 | lr 0.006612
> step 2660: batch_loss 0.4448 | norm 0.1617 | lr 0.006565
> step 2680: batch_loss 0.4383 | norm 0.0832 | lr 0.006517
> step 2700: batch_loss 0.4048 | norm 0.1958 | lr 0.006470
>> step 2700: val_loss 0.4217
> step 2720: batch_loss 0.4237 | norm 0.2395 | lr 0.006422
> step 2740: batch_loss 0.4278 | norm 0.1045 | lr 0.006374
> step 2760: batch_loss 0.4390 | norm 0.0895 | lr 0.006326
> step 2780: batch_loss 0.4218 | norm 0.1133 | lr 0.006278
> step 2800: batch_loss 0.4563 | norm 0.0634 | lr 0.006230
>> step 2800: val_loss 0.4195
> step 2820: batch_loss 0.4150 | norm 0.1689 | lr 0.006182
> step 2840: batch_loss 0.4242 | norm 0.1135 | lr 0.006134
> step 2860: batch_loss 0.4155 | norm 0.1417 | lr 0.006086
> step 2880: batch_loss 0.4338 | norm 0.9063 | lr 0.006037
> step 2900: batch_loss 0.4172 | norm 0.0626 | lr 0.005989
>> step 2900: val_loss 0.4307
> step 2920: batch_loss 0.4198 | norm 0.0564 | lr 0.005940
> step 2940: batch_loss 0.4201 | norm 0.3409 | lr 0.005892
> step 2960: batch_loss 0.4204 | norm 0.0891 | lr 0.005843
> step 2980: batch_loss 0.4110 | norm 0.1666 | lr 0.005795
> step 3000: batch_loss 0.4059 | norm 0.0686 | lr 0.005746
>> step 3000: val_loss 0.4236
> step 3020: batch_loss 0.4331 | norm 0.0816 | lr 0.005697
> step 3040: batch_loss 0.3833 | norm 0.0824 | lr 0.005649
> step 3060: batch_loss 0.4478 | norm 0.1263 | lr 0.005600
> step 3080: batch_loss 0.4230 | norm 0.0849 | lr 0.005551
> step 3100: batch_loss 0.4143 | norm 0.0503 | lr 0.005502
>> step 3100: val_loss 0.4199
> step 3120: batch_loss 0.3946 | norm 0.1105 | lr 0.005454
> step 3140: batch_loss 0.4208 | norm 0.0505 | lr 0.005405
> step 3160: batch_loss 0.4368 | norm 0.2704 | lr 0.005356
> step 3180: batch_loss 0.4434 | norm 0.1552 | lr 0.005308
> step 3200: batch_loss 0.3925 | norm 0.0872 | lr 0.005259
>> step 3200: val_loss 0.4216
> step 3220: batch_loss 0.4269 | norm 0.0539 | lr 0.005210
> step 3240: batch_loss 0.4022 | norm 0.1623 | lr 0.005162
> step 3260: batch_loss 0.4237 | norm 0.1904 | lr 0.005113
> step 3280: batch_loss 0.4084 | norm 0.2611 | lr 0.005064
> step 3300: batch_loss 0.4076 | norm 0.2283 | lr 0.005016
>> step 3300: val_loss 0.4132
> step 3320: batch_loss 0.3925 | norm 0.1300 | lr 0.004967
> step 3340: batch_loss 0.4026 | norm 0.1275 | lr 0.004919
> step 3360: batch_loss 0.4394 | norm 0.1110 | lr 0.004871
> step 3380: batch_loss 0.4084 | norm 0.1543 | lr 0.004823
> step 3400: batch_loss 0.4215 | norm 0.2031 | lr 0.004774
>> step 3400: val_loss 0.4153
> step 3420: batch_loss 0.4148 | norm 0.0915 | lr 0.004726
> step 3440: batch_loss 0.3995 | norm 0.2270 | lr 0.004678
> step 3460: batch_loss 0.4016 | norm 0.1094 | lr 0.004630
> step 3480: batch_loss 0.3982 | norm 0.1574 | lr 0.004583
> step 3500: batch_loss 0.3545 | norm 0.1236 | lr 0.004535
>> step 3500: val_loss 0.4121
> step 3520: batch_loss 0.4355 | norm 0.0925 | lr 0.004487
> step 3540: batch_loss 0.4059 | norm 0.1019 | lr 0.004440
> step 3560: batch_loss 0.3772 | norm 0.0934 | lr 0.004393
> step 3580: batch_loss 0.4039 | norm 0.1546 | lr 0.004346
> step 3600: batch_loss 0.4077 | norm 0.1646 | lr 0.004298
>> step 3600: val_loss 0.4075
> step 3620: batch_loss 0.4097 | norm 0.0945 | lr 0.004252
> step 3640: batch_loss 0.4570 | norm 0.1850 | lr 0.004205
> step 3660: batch_loss 0.4370 | norm 0.1175 | lr 0.004158
> step 3680: batch_loss 0.3755 | norm 0.1365 | lr 0.004112
> step 3700: batch_loss 0.4145 | norm 0.0901 | lr 0.004065
>> step 3700: val_loss 0.4109
> step 3720: batch_loss 0.4164 | norm 0.2607 | lr 0.004019
> step 3740: batch_loss 0.4182 | norm 0.1010 | lr 0.003973
> step 3760: batch_loss 0.3688 | norm 0.1064 | lr 0.003928
> step 3780: batch_loss 0.3794 | norm 0.0887 | lr 0.003882
> step 3800: batch_loss 0.4045 | norm 0.1288 | lr 0.003837
>> step 3800: val_loss 0.4020
> step 3820: batch_loss 0.3992 | norm 0.0957 | lr 0.003791
> step 3840: batch_loss 0.4008 | norm 0.1192 | lr 0.003746
> step 3860: batch_loss 0.3825 | norm 0.1245 | lr 0.003702
> step 3880: batch_loss 0.4063 | norm 0.0794 | lr 0.003657
> step 3900: batch_loss 0.4258 | norm 0.1672 | lr 0.003613
>> step 3900: val_loss 0.4081
> step 3920: batch_loss 0.4162 | norm 0.1993 | lr 0.003569
> step 3940: batch_loss 0.3730 | norm 0.2510 | lr 0.003525
> step 3960: batch_loss 0.3978 | norm 0.1452 | lr 0.003481
> step 3980: batch_loss 0.4310 | norm 0.1491 | lr 0.003438
> step 4000: batch_loss 0.3858 | norm 0.1510 | lr 0.003394
>> step 4000: val_loss 0.4064
> step 4020: batch_loss 0.3917 | norm 0.3175 | lr 0.003351
> step 4040: batch_loss 0.4482 | norm 0.2698 | lr 0.003309
> step 4060: batch_loss 0.4012 | norm 0.2049 | lr 0.003266
> step 4080: batch_loss 0.3955 | norm 0.6806 | lr 0.003224
> step 4100: batch_loss 0.3871 | norm 0.2196 | lr 0.003182
>> step 4100: val_loss 0.4101
> step 4120: batch_loss 0.4359 | norm 0.2455 | lr 0.003140
> step 4140: batch_loss 0.3786 | norm 0.5370 | lr 0.003099
> step 4160: batch_loss 0.4079 | norm 0.1653 | lr 0.003058
> step 4180: batch_loss 0.3781 | norm 0.3612 | lr 0.003017
> step 4200: batch_loss 0.4130 | norm 0.1563 | lr 0.002977
>> step 4200: val_loss 0.4086
> step 4220: batch_loss 0.3641 | norm 1.5212 | lr 0.002936
> step 4240: batch_loss 0.3999 | norm 0.2518 | lr 0.002897
> step 4260: batch_loss 0.4165 | norm 0.1924 | lr 0.002857
> step 4280: batch_loss 0.3833 | norm 0.2573 | lr 0.002818
> step 4300: batch_loss 0.3829 | norm 0.1315 | lr 0.002779
>> step 4300: val_loss 0.3998
> step 4320: batch_loss 0.3721 | norm 0.0899 | lr 0.002740
> step 4340: batch_loss 0.3940 | norm 0.1673 | lr 0.002702
> step 4360: batch_loss 0.4179 | norm 0.1182 | lr 0.002664
> step 4380: batch_loss 0.4151 | norm 0.1171 | lr 0.002626
> step 4400: batch_loss 0.4037 | norm 0.1969 | lr 0.002589
>> step 4400: val_loss 0.3986
> step 4420: batch_loss 0.3840 | norm 0.0917 | lr 0.002552
> step 4440: batch_loss 0.3898 | norm 0.0921 | lr 0.002515
> step 4460: batch_loss 0.3612 | norm 0.1460 | lr 0.002479
> step 4480: batch_loss 0.3923 | norm 0.1162 | lr 0.002443
> step 4500: batch_loss 0.3908 | norm 0.3938 | lr 0.002407
>> step 4500: val_loss 0.4004
> step 4520: batch_loss 0.4279 | norm 0.1044 | lr 0.002372
> step 4540: batch_loss 0.3731 | norm 0.1360 | lr 0.002337
> step 4560: batch_loss 0.3878 | norm 0.1562 | lr 0.002303
> step 4580: batch_loss 0.3662 | norm 0.1925 | lr 0.002268
> step 4600: batch_loss 0.4036 | norm 0.1489 | lr 0.002235
>> step 4600: val_loss 0.3973
> step 4620: batch_loss 0.3632 | norm 0.2849 | lr 0.002201
> step 4640: batch_loss 0.4121 | norm 0.1479 | lr 0.002168
> step 4660: batch_loss 0.3905 | norm 0.2172 | lr 0.002136
> step 4680: batch_loss 0.3956 | norm 0.1793 | lr 0.002104
> step 4700: batch_loss 0.3839 | norm 1.2443 | lr 0.002072
>> step 4700: val_loss 0.3889
> step 4720: batch_loss 0.3696 | norm 0.4526 | lr 0.002040
> step 4740: batch_loss 0.3494 | norm 0.1291 | lr 0.002009
> step 4760: batch_loss 0.3988 | norm 0.1552 | lr 0.001979
> step 4780: batch_loss 0.4191 | norm 0.1659 | lr 0.001949
> step 4800: batch_loss 0.3915 | norm 0.1005 | lr 0.001919
>> step 4800: val_loss 0.3879
> step 4820: batch_loss 0.3826 | norm 0.2383 | lr 0.001890
> step 4840: batch_loss 0.3871 | norm 0.1497 | lr 0.001861
> step 4860: batch_loss 0.3961 | norm 0.2288 | lr 0.001832
> step 4880: batch_loss 0.3874 | norm 0.1914 | lr 0.001804
> step 4900: batch_loss 0.3862 | norm 0.1222 | lr 0.001777
>> step 4900: val_loss 0.3870
> step 4920: batch_loss 0.3941 | norm 0.1409 | lr 0.001750
> step 4940: batch_loss 0.3749 | norm 0.2758 | lr 0.001723
> step 4960: batch_loss 0.3852 | norm 0.1912 | lr 0.001697
> step 4980: batch_loss 0.4363 | norm 0.2020 | lr 0.001671
> step 5000: batch_loss 0.3940 | norm 0.1839 | lr 0.001645
>> step 5000: val_loss 0.3854
> step 5020: batch_loss 0.4017 | norm 0.1486 | lr 0.001620
> step 5040: batch_loss 0.3546 | norm 0.1356 | lr 0.001596
> step 5060: batch_loss 0.3522 | norm 0.5035 | lr 0.001572
> step 5080: batch_loss 0.3669 | norm 0.1808 | lr 0.001548
> step 5100: batch_loss 0.3731 | norm 0.1505 | lr 0.001525
>> step 5100: val_loss 0.3850
> step 5120: batch_loss 0.3515 | norm 0.1746 | lr 0.001503
> step 5140: batch_loss 0.3708 | norm 0.1739 | lr 0.001481
> step 5160: batch_loss 0.3977 | norm 0.1861 | lr 0.001459
> step 5180: batch_loss 0.3649 | norm 0.1079 | lr 0.001438
> step 5200: batch_loss 0.4318 | norm 0.2388 | lr 0.001417
>> step 5200: val_loss 0.3820
> step 5220: batch_loss 0.3702 | norm 0.2035 | lr 0.001397
> step 5240: batch_loss 0.3576 | norm 0.1172 | lr 0.001377
> step 5260: batch_loss 0.3800 | norm 0.1275 | lr 0.001358
> step 5280: batch_loss 0.3568 | norm 0.1903 | lr 0.001339
> step 5300: batch_loss 0.4151 | norm 0.1471 | lr 0.001321
>> step 5300: val_loss 0.3784
> step 5320: batch_loss 0.3434 | norm 0.1575 | lr 0.001303
> step 5340: batch_loss 0.3698 | norm 0.1861 | lr 0.001285
> step 5360: batch_loss 0.3368 | norm 0.1429 | lr 0.001269
> step 5380: batch_loss 0.3648 | norm 0.2111 | lr 0.001252
> step 5400: batch_loss 0.3572 | norm 0.1665 | lr 0.001236
>> step 5400: val_loss 0.3757
> step 5420: batch_loss 0.3937 | norm 0.1392 | lr 0.001221
> step 5440: batch_loss 0.3908 | norm 0.1840 | lr 0.001206
> step 5460: batch_loss 0.3617 | norm 0.2190 | lr 0.001192
> step 5480: batch_loss 0.3836 | norm 0.1698 | lr 0.001178
> step 5500: batch_loss 0.3890 | norm 0.2100 | lr 0.001165
>> step 5500: val_loss 0.3692
> step 5520: batch_loss 0.3453 | norm 0.1746 | lr 0.001152
> step 5540: batch_loss 0.3635 | norm 0.2032 | lr 0.001140
> step 5560: batch_loss 0.3253 | norm 0.1831 | lr 0.001128
> step 5580: batch_loss 0.3454 | norm 0.1233 | lr 0.001116
> step 5600: batch_loss 0.3832 | norm 0.5032 | lr 0.001106
>> step 5600: val_loss 0.3697
> step 5620: batch_loss 0.4149 | norm 0.3665 | lr 0.001095
> step 5640: batch_loss 0.3450 | norm 0.1823 | lr 0.001086
> step 5660: batch_loss 0.3749 | norm 0.1378 | lr 0.001077
> step 5680: batch_loss 0.4052 | norm 0.2871 | lr 0.001068
> step 5700: batch_loss 0.3691 | norm 0.3233 | lr 0.001060
>> step 5700: val_loss 0.3660
> step 5720: batch_loss 0.3811 | norm 0.2852 | lr 0.001052
> step 5740: batch_loss 0.3893 | norm 0.2020 | lr 0.001045
> step 5760: batch_loss 0.3673 | norm 0.1773 | lr 0.001038
> step 5780: batch_loss 0.3646 | norm 0.4790 | lr 0.001032
> step 5800: batch_loss 0.3669 | norm 0.1658 | lr 0.001027
>> step 5800: val_loss 0.3636
> step 5820: batch_loss 0.2870 | norm 0.3427 | lr 0.001022
> step 5840: batch_loss 0.3612 | norm 0.1963 | lr 0.001017
> step 5860: batch_loss 0.3878 | norm 0.1194 | lr 0.001013
> step 5880: batch_loss 0.3300 | norm 0.3231 | lr 0.001010
> step 5900: batch_loss 0.3459 | norm 0.1771 | lr 0.001007
>> step 5900: val_loss 0.3632
> step 5920: batch_loss 0.3766 | norm 0.2228 | lr 0.001004
> step 5940: batch_loss 0.3156 | norm 0.2604 | lr 0.001002
> step 5960: batch_loss 0.3316 | norm 0.1299 | lr 0.001001
> step 5980: batch_loss 0.3486 | norm 0.1715 | lr 0.001000
> step 6000: batch_loss 0.3569 | norm 0.1820 | lr 0.001000
>> step 6000: val_loss 0.3609
